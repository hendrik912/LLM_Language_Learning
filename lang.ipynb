{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Statistics**\n",
    "\n",
    "- amount of unique words seen\n",
    "- amount of root form words seen \n",
    "  - amount of word forms per root form\n",
    "\n",
    "- show the amount learned words in terms of covered frequency area\n",
    "  - histogram\n",
    "  - each bar is color coded based on how much of was seen\n",
    "\n",
    "  - two versions\n",
    "    - root vs forms\n",
    "\n",
    "- Words forgotten curve + area covered\n",
    "- Words learned curve + area covered\n",
    "\n",
    "-> display as spiral which gets filled?\n",
    "\n",
    "  -> Nautilus\n",
    "\n",
    "Motivation and Progress\n",
    "\n",
    "-> display how much the area has increased over time\n",
    "-> streak\n",
    "\n",
    "-----------------\n",
    "\n",
    "**Misc**\n",
    "\n",
    "- What to generate via the LLM\n",
    "  - how to make it cost efficient?\n",
    "    - consider similar contexts to prevent recomputation\n",
    "\n",
    "  - for each word, create a explanation + examples beforehand\n",
    "  - option to generate explanation specifically for this sentence\n",
    "    - explanation is kept and stored for examples and other people that see that exact same sentence\n",
    "  \n",
    "- How to answer questions?\n",
    "- What about active recall?\n",
    "- App purely for reading?\n",
    "\n",
    "  \n",
    "Provide a daily story/article \n",
    "- modify the difficulty of the article given most frequent words\n",
    "  known words, etc. and the context of the article.\n",
    "\n",
    "- quiz on the understanding of the article\n",
    "- keep track of words in article -> add to exposed etc. \n",
    "  update proba. of new words to be learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hendr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hendr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import learning\n",
    "\n",
    "from learning.language import Language\n",
    "from learning.person import Person\n",
    "from learning import utils as learning_utils\n",
    "from learning import statistics\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "Compute frequencies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b0bf8c254f46b7a6f195a8167611bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute sentence structure frequency (not implemented)\n"
     ]
    }
   ],
   "source": [
    "language = Language(\"hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "\n",
    "if plot: \n",
    "    statistics.plot_word_frequencies(language.word_freq, threshold=0.25, figsize=(6,3))\n",
    "    statistics.plot_cumulative_coverage(language.word_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "टॉम नहीं है। \t Tom isn't there.\n",
      "ठीक है। \t Ok.\n",
      "पता है। \t I know.\n",
      "वह नहीं है। \t It isn't her.\n",
      "टॉम अच्छा नहीं है। \t Tom isn't nice.\n",
      "वह स्कूल में है। \t She's at school.\n",
      "मैं टॉम को नहीं दूँगी। \t I won't give it to Tom.\n",
      "टॉम नहीं है। \t It's not Tom.\n",
      "मैं भारत में हूँ। \t I'm in India.\n",
      "वह शहर में नहीं है। \t He is not in the city.\n",
      "वे नहीं हैं। \t It isn't her.\n",
      "टॉम को बुलाते हैं। \t Let's call Tom.\n",
      "मैं तुम से लड़ूंगा। \t I will fight you.\n",
      "मैं रसोई में हूँ। \t I'm in the kitchen.\n",
      "वह तुम हो। \t That is you.\n",
      "टॉम से पूछते हैं। \t Let's go ask Tom.\n",
      "टॉम और मैं दोस्त हैं। \t Tom and I are friends.\n",
      "वह क्या है? \t What's that?\n",
      "मैं ईरान में था। \t I was in Iran.\n",
      "आप ईरान के हैं। \t You're from Iran.\n",
      "मैं पुर्तगाल से हूँ। \t I am from Portugal.\n",
      "टॉम, क्या वह तुम हो? \t Is that you, Tom?\n",
      "मुझे वह नहीं चाहिए। \t I don't want it.\n",
      "हम सिसिली के हैं। \t We are from Sicily.\n",
      "मुझे यह चाहिए था। \t I wanted this.\n",
      "तुम ईरान के हो। \t You're from Iran.\n",
      "मैं सिंगापुर से हूँ। \t I'm from Singapore.\n",
      "टॉम का था। \t It was Tom's.\n",
      "मुझे क्या हो रहा है? \t What's happening to me?\n",
      "टॉम ने एक गलती की। \t Tom made a mistake.\n",
      "हम जल्दी में हैं। \t We're in a hurry.\n",
      "मैं भी कर सकता हूँ। \t I can do it, too.\n",
      "मुझे चाहिए। \t I want it.\n",
      "तुम कर सकते हो। \t You can do it.\n",
      "क्या आप हॉलैंड से हैं? \t Are you from Holland?\n",
      "हो गया। \t Done.\n",
      "उसने एक किसान के बेटी से शादी की। \t He married a farmer's daughter.\n",
      "यह बहुत मुश्किल था। \t It was very difficult.\n",
      "मुझे जर्मनी के बारे में बताओ। \t Tell me about Germany.\n",
      "अपने आप को जानो। \t Know yourself.\n",
      "यह रामू को दो। \t Give this to Ramu.\n",
      "और एक बार। \t One more time.\n",
      "वह नहीं था। \t It wasn't him.\n",
      "तुम्हें हो क्या गया था? \t What came over you?\n",
      "मैंने दोस्तों से बात की। \t I talked to friends.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m sentence_lengths_eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      7\u001b[0m     sentence, translation \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mperson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_lengths_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_lengths_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msentence_random_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_random_eps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, translation)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\learning\\person.py:68\u001b[0m, in \u001b[0;36mPerson.choose_sentence\u001b[1;34m(self, word_freq_eps, word_seen_freq_eps, sentence_lengths_eps, sentence_structure_freq_eps, sentence_random_eps)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# scores based on word seen frequencies\u001b[39;00m\n\u001b[0;32m     66\u001b[0m word_seen_scores \u001b[38;5;241m=\u001b[39m ranking\u001b[38;5;241m.\u001b[39mwords_seen_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_seen_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepetition_threshold)\n\u001b[1;32m---> 68\u001b[0m sentence_words_scores \u001b[38;5;241m=\u001b[39m \u001b[43mranking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_words_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_freq_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_freq_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_seen_freq_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_seen_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# -------------------------------------------\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# sentence scores -- structure\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# >>> does this also affect the choice of the length of a sentence?\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_structure_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\learning\\ranking.py:72\u001b[0m, in \u001b[0;36msentence_words_scores\u001b[1;34m(sentences, word_freq_eps, word_freq_scores, word_seen_freq_eps, word_seen_scores)\u001b[0m\n\u001b[0;32m     68\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m---> 72\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_up_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(words)\n\u001b[0;32m     75\u001b[0m     rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\learning\\utils.py:39\u001b[0m, in \u001b[0;36mclean_up_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_up_sentence\u001b[39m(sentence):\n\u001b[1;32m---> 39\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\hendr\\Documents\\GitProjects\\SourceLocalisation\\.venv\\lib\\site-packages\\nltk\\tokenize\\destructive.py:183\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m--> 183\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# We are not using CONTRACTIONS4 since\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# they are also commented out in the SED scripts\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# for regexp in self._contractions.CONTRACTIONS4:\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#     text = regexp.sub(r' \\1 \\2 \\3 ', text)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "person = Person(language, repetition_threshold=10)\n",
    "\n",
    "sentence_random_eps = 0.75\n",
    "sentence_lengths_eps = 0.0\n",
    "\n",
    "for i in range(0, 50):\n",
    "    sentence, translation = \\\n",
    "        person.choose_sentence(sentence_lengths_eps=sentence_lengths_eps, \n",
    "                            sentence_random_eps=sentence_random_eps)\n",
    "    print(sentence,\"\\t\", translation)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# after each sentece\n",
    "# - print total coverage of frequency\n",
    "#   - plot as area in curve o.Ä.\n",
    "# - print total number of words now known\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    " - seen words: dont use hard threshold but decrease score using\n",
    "   sigmoid or something like that?\n",
    "   higher frequency less impact but still some impact?\n",
    "     \n",
    " - in the loop:\n",
    "  - decrease seen word counters when they were not seen in a while?\n",
    "  \n",
    "\n",
    "- what about words that occured but are a bit rare?\n",
    "  boost proba. in order to keep them in mind?\n",
    "\n",
    "- one-to-many relation?\n",
    "\n",
    "- randomness: choose from prob dist. of best n sentences?\n",
    "\n",
    "------------\n",
    "\n",
    "Sentence Structure\n",
    "\n",
    "\n",
    "\n",
    "LLM explanations\n",
    "\n",
    "- for every word (not root form)\n",
    "  - take some examples sentences\n",
    "  - let LLM explain why the word is used as it is used\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
